library(readr)
train <- read_csv("~/GitHub/Kaggle-Projects/Quora Question Pairs/train.csv/train.csv")
View(train)
colnames(train)
train.id <- as.factor(train.id)
train.qid1 <- as.factor(train.qid1)
train$id <- as.factor(train$id)
train$qid1 <- as.factor(train$qid1)
train$qid2 <- as.factor(train$qid2)
train$is_duplicate <- as.factor(train$is_duplicate)
str(train)
library(dplyr)
library(tidytext)
library(tidyr)
library(tidyverse)
install.packages("tidytext")
f.word.count <- function(my.list) { sum(stringr::str_count(my.list, "\\S+")) }
my.list <- list(train = train)
sample.df <- data.frame(text.source = train,
line.count = NA, word.count = NA)
set.seed(324)
percent <- 0.05
randoms <- lapply(my.list, function (x) rbinom(x, 1, percent))
sample.list <- list(train = NA)
for (i in 1:length(my.list)) {
sample.list[[i]] <- my.list[[i]][randoms[[i]] == 1]
}
sample.df$line.count <- sapply(sample.list, length)
sample.df$word.count <- sapply(sample.list, f.word.count)
removeURL <- function(x) gsub("http:[[:alnum:]]*", "", x)
removeHashTags <- function(x) gsub("#\\S+", "", x)
removeTwitterHandles <- function(x) gsub("@\\S+", "", x)
removeQuestionMarks <- function(x) gsub("?","",x)
text.corpus <- tm::Corpus(VectorSource(sample.list))
qsn1_bigrams <- train$question1 %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
qsn1_bigrams <- train$question1 %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
library(magrittr)
qsn1_bigrams <- train$question1 %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
install.packages("widyr")
library(widyr)
library(dplyr)
library(tidytext)
library(tidyr)
library(tidyverse)
library(magrittr)
library(widyr)
qsn1_bigrams <- train$question1 %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
qsn1_bigrams <- train %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
library(janeaustenr)
austen_books()
qsn1_bigrams <- train %>%
unnest_tokens(bigram, question1, token = "ngrams", n = 2)
View(qsn1_bigrams)
qsn2_bigrams <- train %>%
unnest_tokens(bigram, question2, token = "ngrams", n = 2)
my.corpus <- tm_map(train$question1, removePunctuation)
require("tm")
my.corpus <- tm_map(train$question1, removePunctuation)
my.corpus <- tm_map(train, removePunctuation)
x=train$question1
x=as.character(x)
x=tolower(x)
x=removePunctuation(x)
x=removeNumbers(x)
stopWords = stopwords("en")
extrastop = c("rt","a","the","is","an","was","can","its","via","will","for")
x=removeWords(x,stopWords)
x=removeWords(x,extrastop)
y=train$question2
y=as.character(y)
y=tolower(y)
y=removePunctuation(y)
y=removeNumbers(y)
stopWords = stopwords("en")
extrastop = c("rt","a","the","is","an","was","can","its","via","will","for")
y=removeWords(y,stopWords)
y=removeWords(y,extrastop)
df$question1 = x
train$question1 = x
train$question2 = y
View(train)
train2 <- read_csv("~/GitHub/Kaggle-Projects/Quora Question Pairs/train.csv/train.csv")
View(train2)
tr_qsn1M <- create_matrix(train["question1"],language="english", removeStopwords=FALSE)
my.tdm <- TermDocumentMatrix(train$question1)
library(tm)
tr_qsn1M <- create_matrix(train["question1"],language="english", removeStopwords=FALSE)
library(NLP)
tr_qsn1M <- create_matrix(train["question1"],language="english", removeStopwords=FALSE)
library(RTextTools)
tr_qsn1M <- create_matrix(train["question1"],language="english", removeStopwords=FALSE)
library(caret)
tr_qsn1M <- create_matrix(train["question1"],language="english", removeStopwords=FALSE)
library(ROSE)
tr_qsn1M <- create_matrix(train["question1"],language="english", removeStopwords=FALSE)
library(RTextTools)
library("RTextTools", lib.loc="~/R/win-library/3.3")
tr_qsn1M <- create_matrix(train["question1"],language="english", removeStopwords=FALSE)
qsn1_bigrams_separated <- qsn1_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
qsn2_bigrams_separated <- qsn2_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
train$question1<-as.character(train$question1)
train$question2<-as.character(train$question2)
library(stringdist)
install.packages("stringdist")
library(stringdist)
distance.methods<-c('osa','lv','dl','hamming','lcs','qgram','cosine','jaccard','jw')
dist.methods<-list()
for(m in 1:length(distance.methods))
{
dist.name.enh<-matrix(NA, ncol = length(train$question2),nrow = length(train$question1))
for(i in 1:length(train$question2)) {
for(j in 1:length(train$question1)) {
dist.name.enh[j,i]<-stringdist(tolower(train[i,]$question2),tolower(train[j,]$question1),method = distance.methods[m])
#adist.enhance(train[i,]$question2,train[j,]$question1)
}
}
dist.methods[[distance.methods[m]]]<-dist.name.enh
}
distance.methods<-c('osa')
dist.methods<-list()
for(m in 1:length(distance.methods))
{
dist.name.enh<-matrix(NA, ncol = length(train$question2),nrow = length(train$question1))
for(i in 1:length(train$question2)) {
for(j in 1:length(train$question1)) {
dist.name.enh[j,i]<-stringdist(tolower(train[i,]$question2),tolower(train[j,]$question1),method = distance.methods[m])
#adist.enhance(train[i,]$question2,train[j,]$question1)
}
}
dist.methods[[distance.methods[m]]]<-dist.name.enh
}
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
bigrams_filtered <- qsn1_bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
View(bigrams_filtered)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
View(bigrams_united)
source('~/.active-rstudio-document', encoding = 'UTF-8')
install.packages("h2o")
# This is an example of H2O's recently added word2vec model which is loosely based
# on the script found here: https://github.com/h2oai/h2o-3/blob/master/h2o-r/demos/rdemo.word2vec.craigslistjobtitles.R
library(data.table)
library(h2o)
h2o.init(nthreads = -1)
ts1 <- fread("../input/train.csv", select=c("id","question1","question2","is_duplicate"))
print("Some question cleanup")
# It is important to remove "\n" -- it appears to cause a parsing error when converting to an H2OFrame
ts1[,":="(question1=gsub("'|\"|'|“|”|\"|\n|,|\\.|…|\\?|\\+|\\-|\\/|\\=|\\(|\\)|‘", "", question1),
question2=gsub("'|\"|'|“|”|\"|\n|,|\\.|…|\\?|\\+|\\-|\\/|\\=|\\(|\\)|‘", "", question2))]
ts1[,":="(question1=gsub("  ", " ", question1),
question2=gsub("  ", " ", question2))]
print("get list of unique questions")
# Using only questions from the training set because the test set has 'questions' that are fake
questions <- as.data.table(rbind(ts1[,.(question=question1)], ts1[,.(question=question2)]))
questions <- unique(questions)
questions.hex <- as.h2o(questions, destination_frame = "questions.hex", col.types=c("String"))
STOP_WORDS = c("ax","i","you","edu","s","t","m","subject","can","lines","re","what",
"there","all","we","one","the","a","an","of","or","in","for","by","on",
"but","is","in","a","not","with","as","was","if","they","are","this","and","it","have",
"from","at","my","be","by","not","that","to","from","com","org","like","likes","so")
tokenize <- function(sentences, stop.words = STOP_WORDS) {
tokenized <- h2o.tokenize(sentences, "\\\\W+")
# convert to lower case
tokenized.lower <- h2o.tolower(tokenized)
# remove short words (less than 2 characters)
tokenized.lengths <- h2o.nchar(tokenized.lower)
tokenized.filtered <- tokenized.lower[is.na(tokenized.lengths) || tokenized.lengths >= 2,]
# remove words that contain numbers
tokenized.words <- tokenized.lower[h2o.grep("[0-9]", tokenized.lower, invert = TRUE, output.logical = TRUE),]
# remove stop words
tokenized.words[is.na(tokenized.words) || (! tokenized.words %in% STOP_WORDS),]
}
print("Break questions into sequence of words")
words <- tokenize(questions.hex$question)
print("Build word2vec model")
vectors <- 10 # Only 10 vectors to save time & memory
w2v.model <- h2o.word2vec(words
, model_id = "w2v_model"
, vec_size = vectors
, min_word_freq = 5
, window_size = 5
, init_learning_rate = 0.025
, sent_sample_rate = 0
, epochs = 1) # only a one epoch to save time
h2o.rm('questions.hex') # no longer needed
print("Sanity check - find synonyms for the word 'water'")
print(h2o.findSynonyms(w2v.model, "water", count = 5))
print("Get vectors for each question")
question_all.vecs <- h2o.transform(w2v.model, words, aggregate_method = "AVERAGE")
print("Convert to data.table & merge results")
# Could do the rest of these steps in H2O but I'm a data.table addict
question_all.vecs <- as.data.table(question_all.vecs)
questions_all <- cbind(questions, question_all.vecs)
ts1 <- merge(ts1, questions_all, by.x="question1", by.y="question", all.x=TRUE, sort=FALSE)
ts1 <- merge(ts1, questions_all, by.x="question2", by.y="question", all.x=TRUE, sort=FALSE)
colnames(ts1)[5:ncol(ts1)] <- c(paste0("q1_vec_C", 1:vectors), paste0("q2_vec_C", 1:vectors))
print("output question vectors")
fwrite(ts1, "./h2ow2v_vectors.csv")
library(data.table)
library(h2o)
h2o.init(nthreads = -1)
ts1 <- fread("../input/train.csv", select=c("id","question1","question2","is_duplicate"))
print("Some question cleanup")
ts1 <- fread("../train.csv/train.csv", select=c("id","question1","question2","is_duplicate"))
ts1 <- fread("C:/Users/vvrsk/Desktop/Kaggle/Quora/input/train.csv", select=c("id","question1","question2","is_duplicate"))
print("Some question cleanup")
ts1[,":="(question1=gsub("'|\"|'|“|”|\"|\n|,|\\.|…|\\?|\\+|\\-|\\/|\\=|\\(|\\)|‘", "", question1),
question2=gsub("'|\"|'|“|”|\"|\n|,|\\.|…|\\?|\\+|\\-|\\/|\\=|\\(|\\)|‘", "", question2))]
ts1[,":="(question1=gsub("  ", " ", question1),
question2=gsub("  ", " ", question2))]
print("get list of unique questions")
questions <- as.data.table(rbind(ts1[,.(question=question1)], ts1[,.(question=question2)]))
questions <- unique(questions)
questions.hex <- as.h2o(questions, destination_frame = "questions.hex", col.types=c("String"))
STOP_WORDS = c("ax","i","you","edu","s","t","m","subject","can","lines","re","what",
"there","all","we","one","the","a","an","of","or","in","for","by","on",
"but","is","in","a","not","with","as","was","if","they","are","this","and","it","have",
"from","at","my","be","by","not","that","to","from","com","org","like","likes","so")
tokenize <- function(sentences, stop.words = STOP_WORDS) {
tokenized <- h2o.tokenize(sentences, "\\\\W+")
# convert to lower case
tokenized.lower <- h2o.tolower(tokenized)
# remove short words (less than 2 characters)
tokenized.lengths <- h2o.nchar(tokenized.lower)
tokenized.filtered <- tokenized.lower[is.na(tokenized.lengths) || tokenized.lengths >= 2,]
# remove words that contain numbers
tokenized.words <- tokenized.lower[h2o.grep("[0-9]", tokenized.lower, invert = TRUE, output.logical = TRUE),]
# remove stop words
tokenized.words[is.na(tokenized.words) || (! tokenized.words %in% STOP_WORDS),]
}
print("Break questions into sequence of words")
words <- tokenize(questions.hex$question)
print("Build word2vec model")
vectors <- 10 # Only 10 vectors to save time & memory
w2v.model <- h2o.word2vec(words
, model_id = "w2v_model"
, vec_size = vectors
, min_word_freq = 5
, window_size = 5
, init_learning_rate = 0.025
, sent_sample_rate = 0
, epochs = 1) # only a one epoch to save time
h2o.rm('questions.hex') # no longer needed
print("Sanity check - find synonyms for the word 'water'")
print(h2o.findSynonyms(w2v.model, "water", count = 5))
print("Get vectors for each question")
question_all.vecs <- h2o.transform(w2v.model, words, aggregate_method = "AVERAGE")
print("Convert to data.table & merge results")
question_all.vecs <- as.data.table(question_all.vecs)
questions_all <- cbind(questions, question_all.vecs)
ts1 <- merge(ts1, questions_all, by.x="question1", by.y="question", all.x=TRUE, sort=FALSE)
ts1 <- merge(ts1, questions_all, by.x="question2", by.y="question", all.x=TRUE, sort=FALSE)
colnames(ts1)[5:ncol(ts1)] <- c(paste0("q1_vec_C", 1:vectors), paste0("q2_vec_C", 1:vectors))
print("output question vectors")
fwrite(ts1, "./h2ow2v_vectors.csv")
View(ts1)
w2v.model
w2v.model$cross_validation_models
modeltest<- as.factor(w2v.model$cross_validation_models)
modeltest<- as.data.frame(w2v.model$cross_validation_models)
print(h2o.findSynonyms(w2v.model, "france", count = 5))
print(h2o.findSynonyms(w2v.model, "how", count = 5))
print(h2o.findSynonyms(w2v.model, "what", count = 5))
print(h2o.findSynonyms(w2v.model, "kohinoor", count = 5))
print(h2o.findSynonyms(w2v.model, "shek", count = 5))
print(h2o.findSynonyms(w2v.model, "what is", count = 5))
print(h2o.findSynonyms(w2v.model, "shallow", count = 5))
print(h2o.findSynonyms(w2v.model, "skin", count = 5))
View(questions_all)
View(questions_all)
View(questions_all)
colnames(ts1)
ts1$qsn1_avg <- rowMeans(ts1[,5:14])
View(ts1)
ts1$qsn2_avg <- rowMeans(ts1[,15:24])
similarity.train =  1 - spatial.distance.cosine(ts1$qsn1_avg,ts1$qsn2_avg)
library(word2vec)
install.packages("word2vec")
library(word2vec)
cos.sim <- function(a,b)
{
A = a
B = b
return( sum(A*B)/sqrt(sum(A^2)*sum(B^2)) )
}
cos.sim(1,1)
cos.sim(1,0)
cos.sim(1,0.00001)
cos.sim(0.1,0.00001)
cos.sim(0.1,0.00001)
cos.sim(0,0.00001)
cos.sim(0.12,30.00001)
cos.sim(0.12,0.13)
cos.sim(0.12123123123,0.13123123123)
cos.sim(-0.12123123123,0.13123123123)
cos.sim(-0.52123123123,0.13123123123)
cos.sim(0.52123123123,0.13123123123)
install.packages("word2vec")
install.packages("tmcn.word2vec", repos="http://R-Forge.R-project.org")
library(tmcn.word2vec)
devtools::install_github("bmschmidt/wordVectors")
